# https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/nuscenes_lidarseg_tutorial.ipynb

import os
import numpy as np
import yaml
# import cv2
from pathlib import Path
from torch.utils import data
from PIL import Image
from pyquaternion import Quaternion
from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud  # , Box
from nuscenes.nuscenes import NuScenes
from nuscenes.utils import splits
from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix
from nuscenes.lidarseg.lidarseg_utils import colormap_to_colors, plt_to_cv2, get_stats, \
    get_labels_in_coloring, create_lidarseg_legend, paint_points_label

map_name_from_general_to_segmentation_class = {
    'human.pedestrian.adult': 'pedestrian',
    'human.pedestrian.child': 'pedestrian',
    'human.pedestrian.wheelchair': 'ignore',
    'human.pedestrian.stroller': 'ignore',
    'human.pedestrian.personal_mobility': 'ignore',
    'human.pedestrian.police_officer': 'pedestrian',
    'human.pedestrian.construction_worker': 'pedestrian',
    'animal': 'ignore',
    'vehicle.car': 'car',
    'vehicle.motorcycle': 'motorcycle',
    'vehicle.bicycle': 'bicycle',
    'vehicle.bus.bendy': 'bus',
    'vehicle.bus.rigid': 'bus',
    'vehicle.truck': 'truck',
    'vehicle.construction': 'construction_vehicle',
    'vehicle.emergency.ambulance': 'ignore',
    'vehicle.emergency.police': 'ignore',
    'vehicle.trailer': 'trailer',
    'movable_object.barrier': 'barrier',
    'movable_object.trafficcone': 'traffic_cone',
    'movable_object.pushable_pullable': 'ignore',
    'movable_object.debris': 'ignore',
    'static_object.bicycle_rack': 'ignore',
    'flat.driveable_surface': 'driveable_surface',
    'flat.other': 'other_flat',
    'flat.sidewalk': 'sidewalk',
    'flat.terrain': 'terrain',
    'static.manmade': 'manmade',
    'static.vegetation': 'vegetation',
    'noise': 'ignore',
    'static.other': 'ignore',
    'vehicle.ego': 'ignore'
}

map_name_from_segmentation_class_to_segmentation_index = {
    'ignore': 0,
    'barrier': 1,
    'bicycle': 2,
    'bus': 3,
    'car': 4,
    'construction_vehicle': 5,
    'motorcycle': 6,
    'pedestrian': 7,
    'traffic_cone': 8,
    'trailer': 9,
    'truck': 10,
    'driveable_surface': 11,
    'other_flat': 12,
    'sidewalk': 13,
    'terrain': 14,
    'manmade': 15,
    'vegetation': 16
}


class Nuscenes(data.Dataset):
    def __init__(self, root,
                 version='v1.0-trainval',
                 split='train',
                 return_ref=False,
                 has_image=True,
                 has_pcd=True,
                 has_label=True):
        assert version in ['v1.0-trainval', 'v1.0-test', 'v1.0-mini']
        if version == 'v1.0-trainval':
            train_scenes = splits.train
            val_scenes = splits.val
        elif version == 'v1.0-test':
            train_scenes = splits.test
            val_scenes = []
        elif version == 'v1.0-mini':
            train_scenes = splits.mini_train
            val_scenes = splits.mini_val
        else:
            raise NotImplementedError
        self.split = split
        self.data_path = root
        self.return_ref = return_ref

        self.nusc = NuScenes(version=version, dataroot=self.data_path, verbose=False)
        self.has_image = has_image

        self.map_name_from_general_index_to_segmentation_index = {}
        for index in self.nusc.lidarseg_idx2name_mapping:
            self.map_name_from_general_index_to_segmentation_index[index] = \
                map_name_from_segmentation_class_to_segmentation_index[
                    map_name_from_general_to_segmentation_class[self.nusc.lidarseg_idx2name_mapping[index]]]

        self.mapped_cls_name = {}
        for v, k in map_name_from_segmentation_class_to_segmentation_index.items():
            self.mapped_cls_name[k] = v

        available_scenes = get_available_scenes(self.nusc)
        available_scene_names = [s['name'] for s in available_scenes]
        train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))
        val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))
        train_scenes = set([available_scenes[available_scene_names.index(s)]['token'] for s in train_scenes])
        val_scenes = set([available_scenes[available_scene_names.index(s)]['token'] for s in val_scenes])

        if self.has_image:
            self.train_token_list, self.val_token_list = get_path_infos_cam_lidar(self.nusc, train_scenes, val_scenes)
        else:
            self.train_token_list, self.val_token_list = get_path_infos_only_lidar(self.nusc, train_scenes, val_scenes)

        print('%s: train scene(%d), val scene(%d)' % (version, len(train_scenes), len(val_scenes)))
        print('%s: train sample(%d), val sample(%d)' % (version, len(self.train_token_list), len(self.val_token_list)))

    def __len__(self):
        'Denotes the total number of samples'
        if self.split == 'train':
            return len(self.train_token_list)
        elif self.split == 'valid':
            return len(self.val_token_list)
        elif self.split == 'test':
            return len(self.train_token_list)

    def __getitem__(self, index):
        if self.split == 'train':
            sample_token = self.train_token_list[index]
        elif self.split == 'valid':
            sample_token = self.val_token_list[index]
        elif self.split == 'test':
            sample_token = self.train_token_list[index]
        else:
            sample_token = ''

        lidar_path = os.path.join(self.data_path, self.nusc.get('sample_data', sample_token)['filename'])
        raw_data = np.fromfile(lidar_path, dtype=np.float32).reshape((-1, 5))
        # print('lidar_path ', lidar_path)
        # print('raw_data ', raw_data.shape)  # n, 5
        # render for certain camera
        self.nusc.render_scene_channel_lidarseg(self.nusc.sample[index]['scene_token'],
                                                'CAM_BACK',
                                                # filter_lidarseg_labels=[18, 28],
                                                verbose=False,  # whether to show
                                                dpi=100,
                                                imsize=(1280, 720),
                                                render_mode='image',
                                                out_folder=os.path.expanduser('/mnt/cephfs/home/lirong/code/debug'))

        # render for all camera
        # nusc.render_scene_lidarseg(my_scene['token'],
        #                            filter_lidarseg_labels=[17, 24],
        #                            verbose=True,
        #                            dpi=100,
        #                            out_path=os.path.expanduser('~/Desktop/my_rendered_scene.avi'))
        if self.split == 'test':
            annotated_data = np.expand_dims(np.zeros_like(raw_data[:, 0], dtype=int), axis=1)
        else:
            self.lidarseg_path = os.path.join(self.data_path, self.nusc.get('lidarseg', sample_token)['filename'])
            annotated_data = np.fromfile(self.lidarseg_path, dtype=np.uint8).reshape((-1, 1))  # label
            # map cls to 0-16
            # print(' unique label ', np.unique(annotated_data))
            annotated_data = np.vectorize(self.map_name_from_general_index_to_segmentation_index.__getitem__)(
                annotated_data)
            # print(' unique label ', np.unique(annotated_data))

        # data_tuple = (raw_data[:, :3], annotated_data)
        # if self.return_ref:
        #     data_tuple += (raw_data[:, 3],)
        # return data_tuple

        pcd = raw_data[:, :3]
        label = annotated_data
        reflection = raw_data[:, 3]
        return pcd, label, reflection

    def change_split(self, s):
        assert s in ['train', 'valid']
        self.split = s

    def parsePathInfoByIndex(self, index):
        return self.lidar_sample_token, ''

    def loadDataByIndex(self, index):
        self.index = index
        if self.has_image:
            self.lidar_sample_token = self.train_token_list[index]['lidar_token']
            self.cam_sample_token = self.train_token_list[index]['cam_token']
        else:
            if self.split == 'train':
                self.lidar_sample_token = self.train_token_list[index]
            elif self.split == 'valid':
                self.lidar_sample_token = self.val_token_list[index]
            elif self.split == 'test':
                self.lidar_sample_token = self.train_token_list[index]

        # self.lidar_record = self.nusc.get('sample_data', self.lidar_sample_token)

        # print('lidar_sample_token ', self.lidar_sample_token)
        self.pointsensor = self.nusc.get('sample_data', self.lidar_sample_token)
        #
        self.lidar_path = os.path.join(self.data_path,
                                       self.nusc.get('sample_data', self.lidar_sample_token)['filename'])
        raw_data = np.fromfile(self.lidar_path, dtype=np.float32).reshape((-1, 5))
        # print('lidar_path ', self.lidar_path)
        # print('raw_data ', raw_data.shape)  # n, 5
        # render for certain camera
        # self.nusc.render_scene_channel_lidarseg(self.nusc.sample[index]['scene_token'],
        #                                         'CAM_BACK',
        #                                         filter_lidarseg_labels=[18, 28],
        # verbose=False,  # whether to show
        # dpi=100,
        # imsize=(1280, 720),
        # render_mode='image',
        # out_folder=os.path.expanduser('/mnt/cephfs/home/lirong/code/debug'))
        #
        # render for all camera
        # nusc.render_scene_lidarseg(my_scene['token'],
        #                            filter_lidarseg_labels=[17, 24],
        #                            verbose=True,
        #                            dpi=100,
        #                            out_path=os.path.expanduser('~/Desktop/my_rendered_scene.avi'))
        if self.split == 'test':
            self.lidarseg_path = None
            annotated_data = np.expand_dims(np.zeros_like(raw_data[:, 0], dtype=int), axis=1)
        else:
            self.lidarseg_path = os.path.join(self.data_path,
                                              self.nusc.get('lidarseg', self.lidar_sample_token)['filename'])
            annotated_data = np.fromfile(self.lidarseg_path, dtype=np.uint8).reshape((-1, 1))  # label

        # data_tuple = (raw_data[:, :3], annotated_data)
        # if self.return_ref:
        #     data_tuple += (raw_data[:, 3],)
        # return data_tuple

        pointcloud = raw_data[:, :4]
        sem_label = annotated_data
        inst_label = np.zeros(pointcloud.shape[0], dtype=np.int32)
        return pointcloud, sem_label, inst_label

    def labelMapping(self, sem_label):
        # map cls to 0-16
        # print(' unique label ', np.unique(sem_label))
        sem_label = np.vectorize(self.map_name_from_general_index_to_segmentation_index.__getitem__)(
            sem_label)  # n, 1
        # print(' unique label ', np.unique(sem_label))
        assert sem_label.shape[-1] == 1
        sem_label = sem_label[:, 0]
        return sem_label

    def loadImage(self, index):
        # sample_record = self.nusc.get('sample', sample_token)
        self.cam_sample_token = self.train_token_list[index]['cam_token']
        self.cam = self.nusc.get('sample_data', self.cam_sample_token)
        image = Image.open(os.path.join(self.nusc.dataroot, self.cam['filename']))
        return image

    def getColorMap(self):
        '''
        useage: coloring = colors[points_label]
        :return: A numpy array which has length equal to the number of points in the pointcloud, and each value is
             a RGBA array.
        '''
        colors = colormap_to_colors(self.nusc.colormap, self.nusc.lidarseg_name2idx_mapping)  # Shape: [num_class, 3]
        return colors

    def mapLidar2Camera(self,
                        seq_id,
                        pointcloud,
                        img_h,
                        img_w,
                        min_dist: float = 1.0,
                        show_lidarseg: bool = True,  # False
                        render_intensity: bool = True,  # False
                        filter_lidarseg_labels=None,
                        vis_render_img=False
                        ):
        assert self.pointsensor['is_key_frame'], \
            'Error: Only pointclouds which are keyframes have lidar segmentation labels. Rendering aborted.'
        assert self.pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render lidarseg labels for lidar, ' \
                                                               'not %s!' % self.pointsensor['sensor_modality']

        # Projects a pointcloud into a camera image along with the lidarseg labels
        cam = self.cam
        pointsensor = self.pointsensor
        pcl_path = os.path.join(self.nusc.dataroot, pointsensor['filename'])

        pc = LidarPointCloud.from_file(pcl_path)

        # im = Image.open(os.path.join(self.nusc.dataroot, cam['filename']))

        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.
        # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.
        cs_record = self.nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])
        pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)
        pc.translate(np.array(cs_record['translation']))

        # Second step: transform from ego to the global frame.
        poserecord = self.nusc.get('ego_pose', pointsensor['ego_pose_token'])
        pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)
        pc.translate(np.array(poserecord['translation']))

        # Third step: transform from global into the ego vehicle frame for the timestamp of the image.
        poserecord = self.nusc.get('ego_pose', cam['ego_pose_token'])
        pc.translate(-np.array(poserecord['translation']))
        pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)

        # Fourth step: transform from ego into the camera.
        cs_record = self.nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])
        pc.translate(-np.array(cs_record['translation']))
        pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)

        # Fifth step: actually take a "picture" of the point cloud.
        # Grab the depths (camera frame z axis points away from the camera).
        depths = pc.points[2, :]

        # Take the actual picture (matrix multiplication with camera-matrix + renormalization).
        points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)

        # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
        # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera
        # casing for non-keyframes which are slightly out of sync.
        mask = np.ones(depths.shape[0], dtype=bool)
        mask = np.logical_and(mask, depths > min_dist)
        mask = np.logical_and(mask, points[0, :] > 1)
        mask = np.logical_and(mask, points[0, :] < img_h - 1)
        # mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)
        mask = np.logical_and(mask, points[1, :] > 1)
        mask = np.logical_and(mask, points[1, :] < img_w - 1)
        # mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)
        # points = points[:, mask] # 3, n
        # coloring = coloring[mask] # n, 3

        if vis_render_img:
            raise NotImplementedError
            # if self.lidarseg_path is None:
            #     # 1. Retrieve the color from the depth.
            #     coloring = depths  # n,
            # else:
            #     # 2. Retrive the color from the seg label
            #     coloring = paint_points_label(self.lidarseg_path, filter_lidarseg_labels,
            #                                   self.nusc.lidarseg_name2idx_mapping, self.nusc.colormap)  # n, 3
            # im = self.loadImage(self.index)
            # # Converts a scatter plot in matplotlib to an image in cv2.
            # mat = plt_to_cv2(points[:, mask], coloring[mask], im, (img_w, img_h),
            #                  dpi=100)  # dtype('uint8') (1600, 900, 3)
            # cv2.imwrite(
            #     '/mnt/cephfs/home/lirong/code/poincloudProcessor/debug_temp/render_img_{}.jpg'.format(self.index), mat)

        mapped_points = points.transpose(1, 0)  # n, 3
        mapped_points = np.fliplr(mapped_points[:, :2])

        # fliplr so that indexing is row, col and not col, row
        return mapped_points[mask, :], mask  # (3, n) (n, )


def get_available_scenes(nusc):
    # only for check if all the files are available
    available_scenes = []
    # print('total scene num:', len(nusc.scene))
    for scene in nusc.scene:
        scene_token = scene['token']
        scene_rec = nusc.get('scene', scene_token)
        sample_rec = nusc.get('sample', scene_rec['first_sample_token'])
        sd_rec = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])
        has_more_frames = True
        scene_not_exist = False
        while has_more_frames:
            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec['token'])
            if not Path(lidar_path).exists():
                scene_not_exist = True
                break
            else:
                break

        if scene_not_exist:
            continue
        available_scenes.append(scene)
    # print('exist scene num:', len(available_scenes))
    return available_scenes


def get_path_infos_only_lidar(nusc, train_scenes, val_scenes):
    train_lidar_token_list = []
    val_lidar_token_list = []
    for sample in nusc.sample:
        scene_token = sample['scene_token']
        lidar_token = sample['data']['LIDAR_TOP']

        if scene_token in train_scenes:
            train_lidar_token_list.append(lidar_token)
        else:
            val_lidar_token_list.append(lidar_token)
    return train_lidar_token_list, val_lidar_token_list


def get_path_infos_cam_lidar(nusc, train_scenes, val_scenes):
    train_lidar_token_list = []
    val_lidar_token_list = []

    for sample in nusc.sample:
        scene_token = sample['scene_token']
        lidar_token = sample['data']['LIDAR_TOP']  # 360°的lidar

        if scene_token in train_scenes:
            for i in ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT']:
                a = {'lidar_token': lidar_token, 'cam_token': sample['data'][i]}
                train_lidar_token_list.append(a)
        else:
            for i in ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT']:
                a = {'lidar_token': lidar_token, 'cam_token': sample['data'][i]}
                val_lidar_token_list.append(a)
    return train_lidar_token_list, val_lidar_token_list


if __name__ == '__main__':
    # data_path = '/mnt/cephfs/dataset/pointclouds/nuscenes/lidarseg'
    data_path = '/mnt/cephfs/dataset/pointclouds/nuscenes'
    # dataset = Nuscenes(root=data_path, version='v1.0-trainval', split='train', return_ref=False)
    dataset = Nuscenes(root='/mnt/cephfs/home/lirong/data/nuscenes', version='v1.0-mini', split='train',
                       return_ref=False)
    data = dataset.loadDataByIndex(index=10)
